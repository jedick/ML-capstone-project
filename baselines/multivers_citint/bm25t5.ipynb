{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get context.json\n",
    "def get_context(file_path):\n",
    "    output_path = file_path[:file_path.rfind(\"/\")]\n",
    "    contexts = {}\n",
    "    for folder in sorted(os.listdir(file_path)):\n",
    "        if \".DS_Store\" in folder:\n",
    "            continue\n",
    "        contexts[f'{folder}'] = {}\n",
    "\n",
    "        for file in sorted(os.listdir(f\"{file_path}/{folder}\")): #folder = pmcid\n",
    "            if \"json\" not in file:\n",
    "                continue\n",
    "            with open(f\"{file_path}/{folder}/{file}\") as f:\n",
    "                data = json.load(f)\n",
    "            start = 1000000000\n",
    "            end = -1\n",
    "            for span in data['citation_context']:\n",
    "                if span['start'] < start:\n",
    "                    start = span['start']\n",
    "                if span['end'] > end:\n",
    "                    end = span['end']\n",
    "            claim = data['citing_paragraph'][start:end].strip().replace('\\n', ' ') # the citation context\n",
    "            if claim != '' and claim is not None:\n",
    "                contexts[f'{folder}'][f'{file}'] = claim\n",
    "            else:\n",
    "                print(f'{folder}{file}')\n",
    "\n",
    "    with open(f'{output_path}/contexts_multivers.json','w') as f:\n",
    "        json.dump(contexts,f,indent=4)\n",
    "\n",
    "get_context(test_file_path)\n",
    "get_context(dev_file_path)\n",
    "get_context(train_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generating sentence folder\n",
    "def generate_sentences_forbm25(path):\n",
    "    result_folder = path[:path.rfind(\"/\")]\n",
    "    print(result_folder)\n",
    "    for file in os.listdir(path):\n",
    "        print(file)\n",
    "        sents = []\n",
    "        with open(f\"{path}/{file}\") as f:\n",
    "            lines = f.readlines()\n",
    "            for line in lines:\n",
    "                line = json.loads(line)\n",
    "                for each_sent in line['sentences']:\n",
    "                    sents.append(each_sent)\n",
    "        with open(f'{result_folder}/Sentences/{file}','w') as f:\n",
    "            json.dump(sents,f,indent=4)\n",
    "\n",
    "generate_sentences_forbm25(test_path)\n",
    "generate_sentences_forbm25(train_path)\n",
    "generate_sentences_forbm25(dev_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import locale\n",
    "import time\n",
    "\n",
    "from pygaggle.rerank.base import Query, Text\n",
    "from pygaggle.rerank.transformer import MonoT5\n",
    "\n",
    "from pyserini.search.lucene import LuceneSearcher\n",
    "import pyserini.index.lucene\n",
    "\n",
    "import subprocess\n",
    "\n",
    "reranker =  MonoT5('castorini/monot5-base-med-msmarco')\n",
    "BM25_k = 60\n",
    "TOP_n = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}\n",
    "outliers = []\n",
    "scores = []\n",
    "DATATYPE = \"Train\"\n",
    "SENTENCE_FOLDER = f\"{DATATYPE}_input_Data/Sentences\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_relevant_sentences(cs, num_bm25=60, num=50):\n",
    "    global results\n",
    "\n",
    "    for pmcid in cs:\n",
    "        # print(pmcid)\n",
    "        curr_ref_file = pmcid\n",
    "\n",
    "        CURR_PMC = pmcid\n",
    "\n",
    "        # if CURR_PMC != '009_PMC3607626':\n",
    "        #     continue\n",
    "        searcher = LuceneSearcher(f'{DATATYPE}_input_Data/indexes/{pmcid}')\n",
    "\n",
    "        results[CURR_PMC] = {}\n",
    "\n",
    "        idx = 0\n",
    "        for file, claim in cs[pmcid].items():\n",
    "            # print(f'Claim {idx}')\n",
    "\n",
    "            idx += 1\n",
    "            hits = searcher.search(claim, k=num_bm25)\n",
    "\n",
    "            if len(hits) == 0:\n",
    "                print(f'Claim {idx}, {claim}')\n",
    "\n",
    "            sentences = []\n",
    "            for i in range(len(hits)):\n",
    "                sentences.append(json.loads(hits[i].raw)['contents'])\n",
    "\n",
    "\n",
    "            texts = [Text(sentences[i], {'sent_idx' : i}, 0) for i in range(len(sentences))]\n",
    "            query = Query(claim)\n",
    "            reranked = reranker.rerank(query, texts)\n",
    "            current_sentences = []\n",
    "            # Print out reranked results:\n",
    "            if (len(reranked) < num):\n",
    "                outliers.append((pmcid, file))\n",
    "\n",
    "            for i in range(0, min(num, len(reranked))):\n",
    "                current_sentences.append((reranked[i].text, reranked[i].score))\n",
    "                scores.append(reranked[i].score)\n",
    "            results[CURR_PMC][file] = current_sentences\n",
    "\n",
    "\n",
    "        with open(f\"Results_{DATATYPE}/results_goldsent_ccsent.json\", 'w') as f:\n",
    "            json.dump(results, f, indent=4)\n",
    "\n",
    "    with open(f\"Results_{DATATYPE}/outliers.txt\",'w') as f1:\n",
    "        json.dump(str(outliers),f1, indent=4)\n",
    "\n",
    "    with open(f\"Results_{DATATYPE}/scores.txt\",'w') as f1:\n",
    "        json.dump(str(sorted(scores)),f1, indent=4)\n",
    "\n",
    "    return\n",
    "\n",
    "\n",
    "\n",
    "def main():\n",
    "\n",
    "    for file in sorted(os.listdir(SENTENCE_FOLDER)):\n",
    "        if \"json\" not in file:\n",
    "            continue\n",
    "        with open(f\"{SENTENCE_FOLDER}/{file}\") as f:\n",
    "            d = json.load(f)\n",
    "            res = []\n",
    "            for i, s in enumerate(d):\n",
    "                curr = {\n",
    "                    \"id\" : f\"sent_{i}\",\n",
    "                    \"contents\" : s\n",
    "                }\n",
    "                res.append(curr)\n",
    "\n",
    "        if not os.path.isdir(f\"{DATATYPE}_input_Data/LuceneDocuments\"):\n",
    "            os.mkdir(f\"{DATATYPE}_input_Data/LuceneDocuments\")\n",
    "        os.mkdir(f\"{DATATYPE}_input_Data/LuceneDocuments/{file[:-5]}\")\n",
    "        with open(f\"{DATATYPE}_input_Data/LuceneDocuments/{file[:-5]}/documents.json\", \"w\") as f:\n",
    "            json.dump(res, f,indent=4)\n",
    "\n",
    "    files = [f for f in os.listdir(SENTENCE_FOLDER) if 'json' in f]\n",
    "\n",
    "\n",
    "    locale.getpreferredencoding = lambda: \"UTF-8\"\n",
    "\n",
    "\n",
    "    if not os.path.isdir(f'{DATATYPE}_input_Data/indexes'):\n",
    "        os.mkdir(f'{DATATYPE}_input_Data/indexes')\n",
    "\n",
    "    os.system('clear') #clean the output of terminal\n",
    "    print(\"Starting Indexing of the files\")\n",
    "\n",
    "    start_indexing = time.time()\n",
    "\n",
    "    for file in files:\n",
    "        command = [\n",
    "        \"python\", \"-m\", \"pyserini.index.lucene\",\n",
    "        \"--collection\", \"JsonCollection\",\n",
    "        \"--input\", f\"{DATATYPE}_input_Data/LuceneDocuments/{file[:-5]}\",\n",
    "        \"--index\", f\"{DATATYPE}_input_Data/indexes/{file[:-5]}\",\n",
    "        \"--generator\", \"DefaultLuceneDocumentGenerator\",\n",
    "        \"--threads\", \"1\",\n",
    "        \"--storePositions\", \"--storeDocvectors\", \"--storeRaw\"\n",
    "        ]\n",
    "        result = subprocess.run(command, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "\n",
    "        # Check if there is an error (returncode != 0)\n",
    "        if result.returncode != 0:\n",
    "            # An error occurred, display the error message (stderr) and any output (stdout)\n",
    "            print(\"Error occurred:\")\n",
    "            print(result.stderr.decode())\n",
    "            print(result.stdout.decode())\n",
    "        print(f\"Completed indexing of {file}\")\n",
    "\n",
    "    end_indexing = time.time()\n",
    "    print(f\"Indexing took {end_indexing-start_indexing:.2f} secs\")\n",
    "\n",
    "    #reranker shifted to top\n",
    "\n",
    "    os.system('clear')\n",
    "\n",
    "    #results = {}\n",
    "\n",
    "    with open(f'{DATATYPE}_input_Data/{DATATYPE}_contexts_goldsent_ccsent.json') as f:\n",
    "        cs = json.load(f)\n",
    "\n",
    "    start_bm25 = time.time()\n",
    "\n",
    "    print(\"Starting Bm25 and T5 Reranker\")\n",
    "    get_relevant_sentences(cs, num_bm25=BM25_k, num=TOP_n)\n",
    "\n",
    "    end_bm25 = time.time()\n",
    "    print(f\"Indexing took {end_indexing-start_indexing:.2f} secs\")\n",
    "    print(f\"Bm_25 and T5 Reranker took {end_bm25-start_bm25:.2f} secs\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
